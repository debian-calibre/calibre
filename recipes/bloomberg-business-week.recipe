from calibre.web.feeds.news import BasicNewsRecipe, classes
from html5_parser import parse
import json
import random
import time

def get_contents(x):
    if x == '':
        return ''
    otype = x.get('type', '')
    if otype == 'text':
        if 'attributes' in x:
            if 'strong' in x['attributes']:
                return '<strong>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</strong>'
            if 'emphasis' in x['attributes']:
                return '<em>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</em>'
            return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
        return x.get('value', '') + ''.join(map(get_contents, x.get('content', '')))
    elif otype == 'br':
        return '<br>'
    elif otype == 'paragraph':
        return '<p>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</p>'
    elif otype == 'heading':
        return '<h3>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</h3>'
    elif otype == 'list':
        return '<ul>' + ''.join(map(get_contents, x.get('content', ''))) + '</ul>'
    elif otype == 'listItem':
        return '<li>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</li>'
    elif otype == 'quote':
        return '<blockquote class="col">' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</blockquote>'
    elif otype == 'media':
        if x['subType'] == 'photo':
            return '<div><div class="img"><img src="{}"></div><div class="cap">{}<div>{}</div></div></div>'.format(
                x['data']['photo']['src'], x['data']['photo']['caption'], x['data']['photo']['credit'])
        elif x['subType'] == 'chart':
            if x['data'] and x['data']['chart']:
                return '<div class="img"><img src="{}"></div>'.format(x['data']['chart']['fallback'])
    elif otype == 'link':
        if 'data' in x:
            if 'href' in x['data']:
                return '<a href="' + x['data']['href'] + '">' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</a>'
            return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
        return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
    elif otype == 'entity':
        if x['subType'] == 'story':
            if x['data'] and x['data']['link'] and x['data']['link']['destination']:
                if 'web' in x['data']['link']['destination']:
                    return '<a href="' + x['data']['link']['destination']['web'] + '">' + x.get('value', '') + ''.join(
                        map(get_contents, x.get('content', ''))) + '</a>'
                return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
            return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
        return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'
    elif otype in {'div', 'callout'}:
        return '<div>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</div>'
    elif not any(x == otype for x in ['', 'ad', 'inline-newsletter', 'tabularData']):
        if any(b in x for b in ['value', 'content']):
            return '<i>' + x.get('value', '') + ''.join(map(get_contents, x.get('content', ''))) + '</i>'

    return ''

class Bloomberg(BasicNewsRecipe):
    title = u'Bloomberg Businessweek'
    language = 'en'
    __author__ = 'unkn0wn'
    no_stylesheets = True
    use_embedded_content = False
    remove_attributes = ['style', 'height', 'width']
    ignore_duplicate_articles = {'url'}
    resolve_internal_links = True
    masthead_url = 'https://assets.bwbx.io/s3/javelin/public/hub/images/BW-Logo-Black-cc9035fbb3.svg'
    description = (
        'Bloomberg Businessweek helps global leaders stay ahead with insights and in-depth analysis on the people,'
        ' companies, events, and trends shaping today\'s complex, global economy.'
    )

    simultaneous_downloads = 1

    extra_css = '''
        .auth {font-size:small; font-weight:bold;}
        .time, .chart, .css--lede-byline, .css--lede-timestamp {font-size:small;}
        .subhead {font-style:italic; color:#404040;}
        em, .col {color:#202020;}
        .cat {font-size:small; color:gray;}
        .news-figure-caption-text, .cap, .img, .css--caption-outer-wrapper {font-size:small; text-align:center;}
        .news-figure-credit {font-size:small; text-align:center; color:#202020;}
    '''

    remove_tags = [
        dict(name=['button', 'svg']),
        dict(name='div', attrs={'id':['bb-that', 'bb-nav']}),
        classes('twitter-logo bb-global-footer __sticky__audio__bar__portal__ css--social-wrapper-outer')
    ]

    def get_browser(self, *a, **kw):
        kw['user_agent'] = 'common_words/based'
        br = BasicNewsRecipe.get_browser(self, *a, **kw)
        br.set_handle_redirect(False)
        return br

    def parse_index(self):
        soup = self.index_to_soup('https://www.bloomberg.com/businessweek')
        bw = soup.find('a', href=lambda x: x and x.startswith('/magazine/businessweek/'))
        edition = 'https://www.bloomberg.com' + bw['href']
        self.log('Downloading ', edition)
        self.cover_url = bw.find('img')['src'].replace('25x19', '600x800')
        soup = self.index_to_soup(edition)
        if timefmt := soup.find(attrs={'class':lambda x: x and x.startswith('styles_MagazineTitle__')}):
            self.timefmt = ' [' + (self.tag_to_string(timefmt).replace(' Issue', '')).strip() + ']'

        feeds = []
        for div in soup.findAll(attrs={'class':lambda x: x and x.startswith(
                ('styles_MagazineFeatures__', 'styles_MagazineStoryList__')
            )}):
            h3 = div.find(attrs={'class':lambda x: x and x.startswith(
                ('styles_featuresTitle__', 'styles_magazineSectionTitle__')
            )})
            sec = self.tag_to_string(h3)
            self.log(sec)
            articles = []
            for art in div.findAll(attrs={'data-component':'headline'}):
                a = art.find('a', href=True)
                url = a['href']
                if url.startswith('http') is False:
                    url = 'https://www.bloomberg.com' + a['href']
                title = self.tag_to_string(a)
                articles.append({'title': title, 'url': url})
                self.log('\t', title, '\n\t\t', url)
            if articles:
                feeds.append((sec, articles))
        return feeds

    def preprocess_raw_html(self, raw, *a):
        root = parse(raw)
        m = root.xpath('//script[@data-component-props="ArticleBody"]')
        if not m:
            m = root.xpath('//script[@data-component-props="FeatureBody"]')
            if not m:
                m2 = root.xpath('//script[@id="__NEXT_DATA__"]')
                if not m2:
                    return raw
        if m:
            data = json.loads(m[0].text)
            data = data['story']

        else:
            data = json.loads(m2[0].text)
            data = data['props']['pageProps']['story']

        title = '<h1>' + data['headline'] + '</h1>'

        cat = subhead = lede = auth = caption = ''

        if 'primaryCategory' in data and data['primaryCategory'] is not None:
                cat = '<p class="cat">' + data['primaryCategory'] + '</p>'

        if len(data['abstract']) != 0 and len(data['abstract']) == 2:
                subhead = '<div class="subhead"><p>' + data['abstract'][0] + '</p><p>' + data['abstract'][1] + '</p></div>'
        else:
            if 'summary' in data:
                subhead = '<div class="subhead"><p>' + data['summary'] + '</p></div>'

        if 'byline' in data and data['byline'] is not None:
                auth = '<div><span class="auth">' + data['byline']\
                 + '</span> | <span class="time">' + data['publishedAt'][:-14] + '</span></div>'

        if 'ledeImageUrl' in data and data['ledeImageUrl'] is not None:
                lede = '<p id="img"><img src="{}">'.format(data['ledeImageUrl'])

        if 'ledeDescription' in data and data['ledeDescription'] is not None:
                caption = '<span class="cap">' + data['ledeDescription'] + '</span>'
        else:
            if 'lede' in data and data['lede'] is not None:
                    if 'alt' in data['lede'] and data['lede']['alt'] is not None:
                            caption = '<span class="cap">' + data['lede']['alt'] + '</span>'

        if m:
            time.sleep(3)
            body = data['body']
        else:
            body = ''
            body_data = data['body']['content']
            for x in body_data:
                body += get_contents(x)
            pause = random.choice((5, 6, 7, 8, 9))
            self.log('Delay: ', pause, ' seconds')
            time.sleep(pause)
        return '<html><body>' + cat + title + subhead + auth + lede + caption + '<div>' + body + '</div></body></html>'

    def preprocess_html(self, soup):
        for icon in soup.findAll('img', attrs={'class':'video-player__play-icon'}):
            icon.decompose()
        for div in soup.findAll('div', attrs={'class':'chart'}):
            nos = div.find('noscript')
            if nos:
                nos.name = 'span'
        for img in soup.findAll('img', attrs={'data-native-src':True}):
            if img['data-native-src'].__contains__('videos') is False:
                img['src'] = img['data-native-src']
            else:
                img['src'] = ''
        for img in soup.findAll('img', attrs={'src':lambda x: x and x.endswith(('-1x-1.jpg', '-1x-1.png'))}):
            img['src'] = img['src'].replace('-1x-1', '750x-1')
        return soup
